<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>ICRA 2025: Workshop on Nonverbal Cues for Human-Robot Cooperative Intelligence</title>
    <link rel="icon" type="image/x-icon" href="images/favicon.ico">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css"> 
    <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css"> 
    <!--  <link rel="stylesheet" type="text/css" href="css/bootstrap.min.css"> --> 
    <link rel="stylesheet" type="text/css" href="css/main.css"> 
</head>
<body>
<nav class="navbar navbar-expand-lg navbar-light bg-light fixed-top shadow">
    <!-- <div class="navbar-title"><a href="#">ICRA 2025</a></div> -->
    <a class="navbar-title d-none d-md-block" href="#">
        <img src="images/logo_icra.png" width="100" >
    </a>
    <div class="container">
        <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent"
                aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse flex-grow-0" id="navbarSupportedContent">
            <ul class="navbar-nav">
                <li class="nav-item">
                    <a class="nav-link" href="#">Home</a>
                </li>                
                <li class="nav-item">
                    <a class="nav-link" href="#submissions">Call for Papers</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="#review">Review Timeline</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="#schedule">Program</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="#speakers">Speakers</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="#motivation">Motivation</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="#organizers">Organizers</a>
                </li>
            </ul>
        </div>
    </div>
    <a class="navbar-right-log d-none d-xl-block" href="https://www.jp.honda-ri.com/en/" target="_blank">
        <img src="images/hrijp_log.png" width="140" height="50">
    </a>
</nav>
<div class="container" style="max-width: 960px; margin-top: 120px;">
    <div class="jumbotron">
        <h1 class="anchor"><span class="highlight1">Workshop</span> on Nonverbal Cues for Human-Robot Cooperative Intelligence</h1>
        <div class="row">
            <div class="col-lg-6 col-xs-9">
                <img src="images/banner_iros.png" class="d-none d-lg-block" style="width:95%; margin-top: 20px; margin-left: 20px;" >
                <img src="images/banner.png" class="d-none d-md-block d-lg-none d-xl-none" style="margin-left: 20px; width: 95%;">
                <div class="banner-info  shadow">
                    <div class="info-entry">
                        <div class="row">
                            <div class="col-md-1" style="margin-right: 8px;">
                                <i class="fa fa-location-dot" style="font-size: 2.7rem; margin-top: 10px;"></i>
                            </div>
                            <div class="col-md-9" style="margin-top: auto; margin-bottom: auto;">
                                Georgia World Congress Center in Atlanta, GA, United States
                            </div>
                        </div>
                    </div>
                    <div class="row">
                        <div class="col-md-6 info-entry">
                            <i class="fa fa-calendar"></i>
                            <!-- -th May -->
                        </div>
                        <div class="col-md-6 info-entry">
                            <i class="fa fa-clock"></i>
                            <!--00:00 to 00:00-->
                        </div>
                    </div>
                </div>
            </div>
            <div class="col-lg-6">
                <img src="images/banner_art.png" class="d-none d-lg-block">                
            </div>
        </div>
        
        <!-- <p></p> -->
    </div>
    <!-- <div class="row">
        <div class="col-xs-12">
            <a class="anchor" id="abstract"></a>
            <h2 class="h1-bullet">Abstract</h2>
        </div>
    </div> -->
    <!-- <div class="row mb-4">
        <div class="col-xs-12">
            
        </div>
    </div> -->

    <div class="row">
        <div class="col-xs-12">
            <a class="anchor" id="topics"></a>
            <h2 class="h1-bullet">About the Workshop</h2>
        </div>
    </div>
    <div class="row mb-4">
        <div class="col-xs-12">
            

                <!-- <div class="video-container">                    
                    <video class="shadow-lg" src="videos/v1.mp4#t=20,180" controls width="700px"></video>
                </div> -->

            <p style="text-indent: 50px;">This workshop is dedicated to discussing computational methods for
                sensing and recognition of nonverbal cues and internal states in the wild to realize cooperative intelligence between humans and
                intelligent systems. We gather researchers from different expertise, yet having the common goal,
                motivation, and resolve to explore and tackle this delicate issue considering the practicality of
                industrial applications. We are calling for papers to discuss novel methods to realize human-robot
                cooperative intelligence by sensing and understanding humans’ behavior, internal states, and to generate
                empathetic interactions. 
            </p>            
            <ul class="topic-list">
                <li>Human internal state inference, e.g., cognitive, emotional, intention models.</li>
                <li>Recognition of nonverbal cues, e.g., gaze and attention, body language, para-language.</li>
                <li>Multi-modal sensing fusion for scene perception.</li>
                <li>Nonverbal behavior generation for robots/agents, e.g., gaze salience, gesture.</li>
                <li>Synchronization of nonverbal and verbal behavior</li>
                <li>Learning algorithms for cooperative intelligence, e.g., imitation learning.</li>
                <li>Generative and adversarial algorithms to enhance human-robot interaction, e.g., LLMs, diffusion
                    models, VLMs.
                </li>
                <li>Empathetic interaction between humans and intelligent systems.</li>
                <li>Robust sensing of facial and body key points.</li>
                <li>Social interaction dynamics modeling, e.g., harmony level, engagements.</li>
                <li>Personalization of intelligent systems from nonverbal cues and trust evaluation.</li>
                <li>Applications of cooperative intelligence in the wild.</li>
            </ul> 
            <p class="mb-0"><b>Keywords:</b> "Human: Face, gaze, body, pose, gesture, movement, attention,
                cognitivestate, emotion state, intention, empathy, Environment: Object"
            </p>
            <p class="mb-0"><b>Secondary subject:</b> "Human-Robot cooperative intelligence", "Nonverbal cues
                recognition from audiovisual", "Human internal state inference from multi-modality", "Vision
                applications and systems", "Human-Object interaction and scene understanding"
            </p>           
        </div>
       
    </div>

    <div class="row mb-4 sponsors-section">
        <div class="col-xl-3 mb-4">
            <h2 class="h1-bullet">Sponsors</h2>
            <img src="images/hrijp_log.png" class="spons-logo">
            <img src="images/coro_logo.png" class="spons-logo">
        </div>
        <div class="col-xl-1 mb-4"></div>
        <div class="col-xl-8 mb-4">
            <h2 class="h1-bullet">Organizers</h2>
            <div class="row">
                <img src="images/tu_delf.jpg"  class="org-lg-logo">
                <img src="images/eth_zurich.png" style="width: 200px !important; margin-top: 35px;" class="org-lg-logo">                
            </div>
            <div class="row" style="margin-top: 20px;">
                <img src="images/kth_royal.png" class="org-logo">             
                <img src="images/u-tokyo_ins.png" class="org-logo">
                <img src="images/u-tokyo_sci.png" class="org-logo" style="width: 120px;">
                <img src="images/u-stuttgart.png" class="org-logo"> 
            </div>          
        </div>
    </div>

    <div class="row">
        <div class="col-xs-12">
            <a class="anchor" id="news"></a>
            <h2 class="h1-bullet">News updates</h2>
        </div>
    </div>

    <div class="col-xs-12">
        <div class="news-table">
            <table class="table">
                <tbody>
                    <tr>
                        <td>Jul 1st</td>
                        <td>Workshop web page was launched.</td>
                    </tr>
                    <tr>
                        <td>Jul 2nd</td>
                        <td>Submission can be made on EasyChair.</td>
                    </tr>
                    <tr>
                        <td>Aug 14th</td>
                        <td>Workshop paper submission deadline is extended to Aug 24th (final) due to multiple requests.</td>
                    </tr>
                    <tr>
                        <td>Sept 25th</td>
                        <td>Workshop webpage was launched.</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>

    <div class="row">
        <div class="col-xs-12">
            <a class="anchor" id="submissions"></a>
            <h2 class="h1-bullet">Call for Papers</h2>
        </div>
    </div>
    <div class="row">
        <div class="col-xs-12">
            <a class="anchor" id="review"></a>
            <h4 class="sub-header">Submission Guidelines</h4>
        </div>
    </div>
    <div class="col-xs-12 content">
        <p>We invite authors to submit unpublished papers (2-4 pages excluding references) to our workshop, to be presented at a workshop session upon acceptance. 
        Submissions will undergo a peer-review process by the workshop's program committee and accepted papers will be invited 
        to present their works at the workshop (<a href="#presentation">see presentation format</a>). </p>        
    </div>   
    
    <div class="row" style="margin-bottom: 10px;">
        <div class="col-md-1"></div>
        <div class="col-md-1 d-none d-md-block">
            <i class="fa-solid fa-quote-left awards-quote"></i>
        </div>
        <div class="col-md-8" style="text-align: center;">
            <b>We are pleased to announce that award will be given to the best paper accepted by this workshop.</b>
        </div>
        <div class="col-md-1 d-none d-md-block">
            <i class="fa-solid fa-quote-right awards-quote"></i>
        </div>
        <div class="col-md-1"></div>
    </div>

    <div class="row">
        <div class="col-xs-12">
            <a class="anchor" id="review"></a>
            <h4 class="sub-header">Important Dates</h4>
        </div>
    </div>
    <div class="row mb-4">
        <div class="col-xs-12">
            <span>To be determined</span>
            <!--ul class="timeline">
                <li>
                    <span class="date-label">June 18, 2024 PST</span>
                    <p>Notification of workshop acceptance</p>
                </li>
            </ul-->
        </div>
    </div>
    <div class="row">
        <div class="col-xs-12">
            <a class="anchor" id="review"></a>
            <h4 class="sub-header">Submission Instructions</h4>
        </div>
    </div>
    <div class="col-xs-12" style="margin-bottom: 20px">
        Please use the IEEE conferences paper format to write your manuscript.
        <ul>
            <li>LaTex and MS Word template: <a target="_blank" href="https://www.ieee.org/conferences/publishing/templates.html">https://www.ieee.org/conferences/publishing/templates.html</a></li>
        </ul>
        <s>Please submit your paper electronically through the workshop's EasyChair submission system.</s>
    </div>
    <!-- div class="col-xs-12">
        <div class="submition-section">
            <a target="_blank" href="https://easychair.org/my/conference?conf=iros2024workshoponno">Submit papers on EasyChair</a>
        </div>
    </div -->

    <div class="row">
        <div class="col-xs-12">
            <a class="anchor" id="presentation"></a>
            <h4 class="sub-header">Presentation Format</h4>
        </div>
    </div>
    <div class="col-xs-12">
        Accepted papers should be presented in three-way presentation approach to foster active participation 
        <ul class="topic-list">
            <li>Spotlight talks (6 mins talk, Q&A in the poster session) </li>
            <li>In-person A0 posters for in-depth discussions</li>
            <li>Short pre-recorded videos (about 2 minutes) to be uploaded on the workshop webpage</li>
        </ul>
    </div>

    <div class="row">
        <div class="col-xs-12">
            <a class="anchor" id="pub_format"></a>
            <h4 class="sub-header">Publication Format</h4>
        </div>
    </div>
    <div class="col-xs-12">
        Authors are recommended to archive their papers and inform workshop organizers once this procedure is completed. <br>
        <ul>
            <li>Link to arXiv: <a href="https://info.arxiv.org/help/submit/index.html" target="_blank">
            https://info.arxiv.org/help/submit/index.html</a> 
            </li>
        </ul>
        Accepted papers which have been archived will be hosted on the workshop webpage.
    </div>

    <div class="row">
        <div class="col-xs-12">
            <a class="anchor" id="schedule"></a>
            <h2 class="h1-bullet">Program</h2>
        </div>
    </div>
    <div class="row mb-4">
        <div class="col-xs-12">
            <p>We plan a half-day event for 4 hours, including talks by two invited speakers, and one interactive
                session. For participants who could not attend in person, we will disseminate the papers and
                pre-recorded videos on our workshop page, which also consists of a comment section for Q&A.</p>
            <!-- ul class="schelude">
                <li>
                    <span class="time-bg">09:00</span>
                    <span class="ribbon">09:02</span> 
                    <span class="schelude-item-content">Welcome and opening remarks (2 mins)</span>
                </li>
                <li>
                    <span class="time-bg">09:02</span>
                    <span class="ribbon">09:42</span> 
                    <span class="schelude-item-content">Invited talk I (40 mins, including 5 mins Q&A)</span>
                </li>
                <li>
                    <span class="time-bg">09:42</span>
                    <span class="ribbon">10:30</span> 
                    <span class="schelude-item-content">Spotlight talks for accepted workshop and invited IROS 2024 papers (6 + 2 papers, 6 mins each)</span>
                </li>
                <li>
                    <span class="time-bg">10:30</span>
                    <span class="ribbon">11:00</span> 
                    <span class="schelude-item-content">Coffee break and poster session (30 mins)</span>
                </li>
                <li>
                    <span class="time-bg">11:00</span>
                    <span class="ribbon">11:40</span> 
                    <span class="schelude-item-content">Invited talk II (40 mins, including 5 mins Q&A)</span>
                </li>
                <li>
                    <span class="time-bg">11:40</span>
                    <span class="ribbon">12:20</span> 
                    <span class="schelude-item-content">Invited talk III (40 mins, including 5 mins Q&A)</span>
                </li>
                <li>
                    <span class="time-bg">12:20</span>
                    <span class="ribbon">12:55</span> 
                    <span class="schelude-item-content">Interactive session (35 mins)</span>
                </li>
                <li>
                    <span class="time-bg">12:55</span>
                    <span class="ribbon">13:00</span> 
                    <span class="schelude-item-content">Awards and closing remarks (5 mins)</span>
                </li>
            </ul-->
        </div>
    </div>
    <!-- div class="row">
        <div class="col-xs-12">
            <a class="anchor" id="spotlight-talks-and-poster"></a>
            <h4 class="sub-header">Spotlight talks & poster</h4>
            <div class="poster">
                <span>BTGenBot: Behavior Tree Generation for Robotic Tasks with Lightweight LLMs</span>
                <span>Izzo, Riccardo Andrea; Bardaro, Gianluca*; Matteucci, Matteo</span>
                <div class="btn-group btn-group-xs" role="group">
                    <a class="btn" href="./pdf/flash-talk+poster/IROS24_0554_2403.12761v1.pdf">PDF</a>
                    <a class="btn" href="./videos/flash-talk+poster/IROS24_0554_VI_i.mp4">video</a>
                </div>
            </div>
            <div class="poster">
                <span>GazeMotion: Gaze-guided Human Motion Forecasting</span>
                <span>Hu, Zhiming*; Schmitt, Syn; Haeufle, Daniel Florian Benedict; Bulling, Andreas</span>
                <div class="btn-group btn-group-xs" role="group">
                    <a class="btn" target="_blank" rel="noopener noreferrer" href="./pdf/flash-talk+poster/IROS24_0825_hu24_gazemotion.pdf">PDF</a>
                    <a class="btn" target="_blank" rel="noopener noreferrer" href="./videos/flash-talk+poster/IROS24_0825_results.mp4">video</a>
                </div>
            </div>
            <div class="poster">
                <span>Are Large Language Models Aligned with People’s Social Intuitions for Human–Robot Interactions?</span>
                <span>Wachowiak, Lennart; Coles, Andrew; Celiktutan, Oya; Canal, Gerard*</span>
                <div class="btn-group btn-group-xs" role="group">
                    <a class="btn" target="_blank" rel="noopener noreferrer" href="./pdf/flash-talk+poster/IROS24_0899_2403.05701v2.pdf">PDF</a>
                    <a class="btn" target="_blank" rel="noopener noreferrer" href="./videos/flash-talk+poster/IROS24_0899_24_IROS1min.mp4">video</a>
                </div>
            </div>
            <div class="poster">
                <span>To Help or Not to Help: LLM-based Attentive Support for Human-Robot Group Interactions</span>
                <span>Tanneberg, Daniel*; Ocker, Felix; Hasler, Stephan; Deigmoeller, Joerg; Belardinelli, Anna; Wang, Chao; Wersing, Heiko; Sendhoff, Bernhard; Gienger, Michael</span>
                <div class="btn-group btn-group-xs" role="group">
                    <a class="btn" target="_blank" rel="noopener noreferrer" href="./pdf/flash-talk+poster/IROS24_2120_2403.12533v2.pdf">PDF</a>
                    <a class="btn" target="_blank" rel="noopener noreferrer" href="./videos/flash-talk+poster/IROS24_2120_VI_i.mp4">video</a>
                </div>
            </div>
            <div class="poster">
                <span>A kinematic model generates non-circular human proxemics zones</span>
                <span>Fanta Camara*, Charles Fox</span>
                <div class="btn-group btn-group-xs" role="group">
                    <a class="btn" target="_blank" rel="noopener noreferrer" href="./pdf/flash-talk+poster/IROS_2024-Workshop_Camara&Fox.pdf">PDF</a>
                </div>
            </div>
            <div class="poster">
                <span>Autonomous Storytelling for Social Robot with Human-Centered Reinforcement Learning</span>
                <span>Zhang, Lei; Zheng, Chuanxiong; Wang, Hui; Gomez, Randy; Nichols, Eric; Li, Guangliang*</span>
                <div class="btn-group btn-group-xs" role="group">
                    <a class="btn" target="_blank" rel="noopener noreferrer" href="./pdf/flash-talk+poster/IROS24_0618_haru.pdf">PDF</a>
                </div>
            </div>
            <div class="poster">
                <span>Contextual Emotion Recognition using Large Vision Language Models</span>
                <span>Etesam, Yasaman*; Yalcin, Ozge; Zhang, Chuxuan; Lim, Angelica</span>
                <div class="btn-group btn-group-xs" role="group">
                    <a class="btn" target="_blank" rel="noopener noreferrer" href="./pdf/flash-talk+poster/IROS24_1014_2405.08992v1.pdf">PDF</a>
                    <a class="btn" target="_blank" rel="noopener noreferrer" href="./videos/flash-talk+poster/IROS24_1014_Contextual-IROS.mp4">video</a>
                </div>
            </div>
            <div class="poster">
                <span>MoVEInt: Mixture of Variational Experts for Learning HRI from Demonstrations</span>
                <span>Vignesh Prasad*, Alap Kshirsagar, Dorothea Koert, Ruth Stock-Homburg, Jan Peters and Georgia Chalvatzaki</span>
            </div>
        </div>
    </div>
    <br />
    <div class="row">
        <div class="col-xs-12">
            <a class="anchor" id="poster"></a>
            <h4 class="sub-header">Poster</h4>
            <div class="poster">
                <span>React to This! How Humans Challenge Interactive Agents Using Nonverbal Behaviors</span>
                <span>Chuxuan Zhang*, Bermet Burkanova, Lawrence H. Kim, Lauren Yip, Ugo Cupcic, Stephane Lallee, Angelica Lim</span>
                <div class="btn-group btn-group-xs" role="group">
                    <a class="btn" target="_blank" rel="noopener noreferrer" href="./pdf/poster/IROS24_1210_2024_IROS_camera_ready.pdf">PDF</a>
                    <a class="btn" target="_blank" rel="noopener noreferrer" href="./videos/poster/IROS24_1210_VI_fi.mp4">video</a>
                </div>
            </div>
            <div class="poster">
                <span>Deep Active Inference for Engagement Recognition in Robot-Assisted Autism Therapy</span>
                <span>Shyrailym Shaldambayeva*, Saparkhan Kassymbekov, Anara Sandygulova and Almas Shintemirov</span>
            </div>
            <div class="poster">
                <span>Human-Centric Robot Navigation: Leveraging Pedestrian Occlusion Patterns for Traversability Analysis in Crowded Indoor Environments</span>
                <span>Jonathan Tay Yu Liang* and Kanji Tanaka</span>
            </div>
        </div>
    </div-->
    <!-- br />
    <div class="row">
        <div class="col-xs-12">
            <a class="anchor" id="interactive-session"></a>
            <h4 class="sub-header">Interactive session</h4>
            <div class="interactive-session">
                <span>Forum on the Impact of Generative AI on Making Emotionally Intelligent Systems. </span>
                <span><b>Moderator</b> Dr. Jouh Yeong Chew, Honda Research Institute Japan</span>
                <h6>Panelists</h6>
                <ul>
                    <li>Professor Yukie Nagai, The University of Tokyo</li>
                    <li>Professor Daisuke Kurabayashi, Tokyo Institute of Technology, Editor-in-Chief for Advanced Robotics</li>
                    <li>Assistant Professor Angelica Lim, Simon Fraser University, Canada</li>
                    <li>Assistant Professor Gerald Canal, King’s College London, UK</li>
                    <li>Professor Eiichi Yoshida, Tokyo University of Science, IEEE Robotics and Automation Society AdCom member</li>
                </ul>
            </div>
        </div>
    </div-->

    <div class="row">
        <div class="col-xs-12">
            <a class="anchor" id="speakers"></a>
            <h2 class="h1-bullet">Invited Speakers</h2>
        </div>
    </div>
    <div class="row mb-4 speaker-row">
        <div class="col-xs-12">
            <p>
                We intend to have speakers from different ethic backgrounds, countries, and career stages.
                Specifically, we confirmed attendance of four speakers.
            </p>
        </div>
        <div class="row mb-4">
            <div class="col-xl-3 mb-4">
                <img src="images/speaker_1.jpg" class="img-speaker shadow">
            </div>
            <div class="col-xl-9 mb-4">
                <h4>Invited Talk I</h4>
                <h3>Co-Existence with Intelligent Machines</h3>
                <div><span>Satoshi Shigemi, </span> Honda Research Institute Japan, Japan.</div>
                <div>Link to website: <a href="http://www.jp.honda-ri.com/en/about/">http://www.jp.honda-ri.com/en/about/</a>
                </div>
                <div class="speaker-content">
                    <b>Abstract</b>
                    <p>At the Honda Research Institute Japan, we are conducting research on collaborative intelligence, human understanding, and robot systems. In recent years, generative AI has emerged, and AI technology is becoming more familiar in people's lives. For people and machines to coexist 24 hours a day, 365 days a year, it is essential for intelligent machine systems to understand people's feelings and act accordingly. To achieve this, we focus on human social activities, and our research targets are interactions between individuals, groups, and communities. We aim to advance people's happiness by helping them become the way they ought to be and enhancing their fulfillment. Here, we will introduce the research technologies at the Honda Research Institute to make people happy.</p>
                    <b>Biography</b>
                    <p>Satoshi Shigemi is the President of Honda Research Institute Japan.
                    Since 1987, he has been conducting research on robots and control systems at Honda R&D Co. In 2000, he was
                    the Senior Chief Engineer and project lead for the research and development of ASIMO,
                    the humanoid robot. He then developed a high-altitude survey robot for the Fukushima
                    Daiichi Nuclear Power Plant. He has published many papers about human-robot interaction.</p>
                </div>
            </div>
        </div>
        <div class="row mb-4">
            <div class="col-xl-3 mb-4">
                <img src="images/speaker_2.png" class="img-speaker shadow">
            </div>
            <div class="col-xl-9 mb-4">
                <h4>Invited Talk II</h4>
                <h3>Emergence of Cooperative Intelligence through Embodied Predictive Processing</h3>
                <div>
                    <!-- <i>To be confirmed </i> -->
                    <span>Yukie Nagai, </span>  The University of Tokyo, Japan.
                </div>
                <div>Link to website: <a href="https://developmental-robotics.jp/en/members/yukie_nagai/">https://developmental-robotics.jp/en/members/yukie_nagai/</a>
                </div>
                <div class="speaker-content">
                    <b>Abstract</b>
                    <p>Embodied predictive processing extends the brain-centered theory of predictive processing by
                        emphasizing the critical role of the body and social interactions in cognitive functions.
                        This approach highlights how prediction error minimization is dynamically mediated across brain,
                        body, and environment. In this talk, we examine how cooperative intelligence can arise within
                        this framework. Through our experiments, we demonstrate how robots, grounded in their
                        sensorimotor experiences, can anticipate and adapt to the actions and emotions of others,
                        illustrating the potential for collaborative learning and human-robot interactions.</p>
                    <b>Biography</b>
                    <p>Yukie Nagai is a Project Professor at the International Research Center for Neurointelligence at
                    the University of Tokyo. She earned her Ph.D. in Engineering from Osaka University in 2004, after 
                    which she worked at the National Institute of Information and Communications Technology, Bielefeld 
                    University, and then Osaka University. Since 2019, she has been leading the Cognitive Developmental 
                    Robotics Lab at the University of Tokyo. Her research encompasses cognitive developmental robotics, 
                    computational neuroscience, and assistive technologies for developmental disorders. Dr. Nagai employs 
                    computational methods to investigate the underlying neural mechanisms involved in social cognitive 
                    development. In acknowledgment of her work, she received the titles of "World's 50 Most Renowned 
                    Women in Robotics" in 2020 and "35 Women in Robotics Engineering and Science" in 2022, among other 
                    recognitions.</p>
                </div>
            </div>
        </div>
        <div class="row mb-4">
            <div class="col-xl-3 mb-4">
                <img src="images/speaker_3.jpg" class="img-speaker shadow">
            </div>
            <div class="col-xl-9 mb-4">
                <h4>Invited Talk III</h4>
                <h3>Multimodal Social Signal Processing for Human-Robot Interaction</h3>
                <div>
                    <span>Angelica Lim, </span> 
                     Simon Fraser University
                </div>
                <div>
                    Link to website: <a href="https://www.sfu.ca/computing/people/faculty/angelicalim.html">https://www.sfu.ca/computing/people/faculty/angelicalim.html</a>
                </div>
                <div class="speaker-content">
                    <b>Abstract</b>
                    <p>Science fiction has long promised us interfaces and robots that interact with us as smoothly as
                        humans do - Rosie the Robot from The Jetsons, C-3PO from Star Wars, and Samantha from Her.
                        Today, interactive robots and voice user interfaces are moving us closer to effortless,
                        human-like interactions in the real world. In this talk, I will discuss the opportunities
                        and challenges in finely analyzing, detecting and generating non-verbal communication in
                        context, including gestures, gaze, auditory signals, and facial expressions. Specifically,
                        I will discuss how we might allow robots and virtual agents to understand human social signals
                        (including emotions, mental states, and attitudes) across cultures as well as recognize and
                        generate expressions with controllability, transparency, and diversity in mind.</p>
                    <b>Biography</b>
                    <p>Dr. Angelica Lim is the Director of the Rosie Lab, and an Assistant Professor in the School of
                    Computing Science at Simon Fraser University (SFU). Previously, she led the Emotion and Expressivity 
                    teams for the Pepper humanoid robot at SoftBank Robotics. She received her B.Sc. in Computing Science 
                    with Artificial Intelligence Specialization from SFU and a Ph.D. and M.Sc. in Computer Science 
                    (Intelligence Science) from Kyoto University, Japan. She and her team have received Best Paper in 
                    Entertainment Robotics and Cognitive Robotics Awards at IROS 2011 and 2022, and Best Demo and LBR at 
                    HRI 2021 and 2023. She has been featured on the BBC, TEDx, hosted a TV documentary on robotics, and 
                    was recently featured in Forbes 20 Leading Women in AI. Her research interests include multimodal 
                        machine learning, affective computing, and human-robot interaction.</p>
                </div>
            </div>
        </div>
    </div>

    <div class="row">
        <div class="col-xs-12">
            <a class="anchor" id="motivation"></a>
            <h2 class="h1-bullet">Motivation and Background</h2>
        </div>
    </div>

    <div class="row">
        <div class="col-xs-12">
            <p style="text-indent: 50px;">Humans can perceive social cues and the interaction context of another human to infer
            the internal states including cognitive and emotional states, empathy, and intention. This
            unique ability to infer internal states leads to effective social interaction between humans
            desirable in many intelligent systems such as collaborative and social robots, and humanmachine
            interaction systems. However, it is challenging for machines to perceive human
            states under noisy real-world settings, which are usually measured by noninvasive sensors.
            Recent works investigating the potential solutions for the estimation of human states under controlled
            conditions using facial features with the off-the-shelf camera by leveraging
            deep learning methods. This workshop aims to bring interdisciplinary researchers across
            computer vision, artificial intelligence, robotics, and human-computer interaction together
            to share current research achievements and discuss future research directions for human
            behavior and state understanding, and their potential application, especially in the wild
            environment. Specifically, we are interested in cognition-aware computing by integrating
            environment contexts and multi-modal nonverbal social cues not limited to gaze interaction, body
            language and para language. More importantly, we extend multi-modal human
            behavior research to infer the internal states of humans. This is a challenging problem yet
            important to realize effective interaction between humans and intelligent systems. </p>

            <p style="text-indent: 50px;">It is desirable for intelligent systems like robots, virtual agents, human-machine
            interfaces to collaborate and interact seamlessly with humans in the era of Industry 5.0, where
            intelligent systems must work alongside humans to perform a variety of tasks anywhere at home,
            factories, offices, transit, etc. The underlying technologies to achieve efficient and intelligent
            collaboration between humans and ubiquitous intelligent systems can be realized by cooperative
            intelligence, spanning interdisciplinary studies between robotics, AI,
            human-robot and -computer interaction, computer vision, cognitive science, etc. </p>

            <p style="text-indent: 50px;">One of the main considerations to achieve cooperative intelligence
            between humans and
            intelligent systems is to enable everyone and everything to know each other well, like how humans can
            trust or infer the implicit internal states like intention, emotion, and cognitive states of each other.
            The importance of empathy to facilitate human-robot interaction has been highlighted in previous studies
            . However, it is difficult for intelligent systems to estimate the internal states of humans
            because they are dependent on the complex social dynamics and environment contexts. This requires
            intelligent systems to be capable of sensing the multi-modal inputs,
            reasoning the underlying abstract knowledge, and generating the corresponding responses to collaborate
            and interact with humans.</p>

            <p style="text-indent: 50px;"> There are many studies on estimating internal states of humans
            through measurements of wearables and
            non-invasive sensors, but it would be difficult to implement these solutions in the wild because of the
            additional sensors to be worn by humans. One promising solution is to use audiovisual data like
            nonverbal behavior cues consisting of gaze interaction, facial expression, body language and
            paralanguage to infer the internal states of humans. Researchers in cognitive and social psychology have
            long advocated that these nonverbal behaviors are subconsciously generated by humans and reflect the
            internal states of humans under different contexts. Some salient examples are the studies on emotion
            recognition using facial and body language in controlled environment. It remains an open question for
            intelligent systems to sense and recognize nonverbal cues and reason the rich underlying internal states
            of humans in the wild and noisy environments. </p>

        </div>
    </div>

    <div class="row">
        <div class="col-xs-12">
            <a class="anchor" id="organizers"></a>
            <h2 class="h1-bullet">Organizers</h2>
        </div>
    </div>
    <div class="row mb-4">
        <div class="col-xl-3 col-md-4 mb-4">
            <div class="organizer-card shadow-sm">
                <h5 class="card-title">Jouh Yeong Chew</h5>
                <p class="card-text">Honda Research Institute Japan</p>
                <a href="mailto:jouhyeong.chew@jp.honda-ri.com">jouhyeong.chew@jp.honda-ri.com</a>
            </div>
        </div>
        <div class="col-xl-3 col-md-4 mb-4">
            <div class="organizer-card shadow-sm">
                <h5 class="card-title">Xucong Zhang</h5>
                <p class="card-text">TU Delft</p>
                <a href="mailto:xucong.zhang@tudelft.nl">xucong.zhang@tudelft.nl</a>
            </div>
        </div>
        <div class="col-xl-3 col-md-4 mb-4">
            <div class="organizer-card shadow-sm">
                <h5 class="card-title">Iolanda Leite</h5>
                <p class="card-text">KTH Royal Institute of Technology</p>
                <a href="mailto:iolanda@kth.se">iolanda@kth.se</a>
            </div>
        </div>        
        <div class="col-xl-3 col-md-4 mb-4">
            <div class="organizer-card shadow-sm">
                <h5 class="card-title">Daisuke Kurabayashi</h5>
                <p class="card-text">Tokyo Institute of Technology</p>
                <a href="mailto:kurabayashi.d.aa@m.titech.ac.jp">kurabayashi.d.aa@m.titech.ac.jp</a>
            </div>
        </div>
        <div class="col-xl-3 col-md-4 mb-4">
            <div class="organizer-card shadow-sm">
                <h5 class="card-title">Eiichi Yoshida</h5>
                <p class="card-text">Tokyo University of Science</p>
                <a href="mailto:eiichi.yoshida@rs.tus.ac.jp">eiichi.yoshida@rs.tus.ac.jp</a>
            </div>
        </div>        
        <div class="col-xl-3 col-md-4 mb-4">
            <div class="organizer-card shadow-sm">
                <h5 class="card-title">Siyu Tang</h5>
                <p class="card-text">ETH Zürich</p>
                <a href="mailto:siyu.tang@inf.ethz.ch">siyu.tang@inf.ethz.ch</a>
            </div>
        </div>
        <div class="col-xl-3 col-md-4 mb-4">
            <div class="organizer-card shadow-sm">
                <h5 class="card-title">Andreas Bulling</h5>
                <p class="card-text">University of Stuttgart</p>
                <a href="mailto:andreas.bulling@vis.uni-stuttgart.de">andreas.bulling@vis.uni-stuttgart.de</a>
            </div>
        </div>        
    </div>
</div>

<footer class="d-flex flex-wrap justify-content-between align-items-center py-3 my-4 border-top">
    <div class="col-md-8 d-flex align-items-center">
        <a target="_blank" href="https://www.jp.honda-ri.com/en/" class="mb-3 me-2 mb-md-0 text-muted text-decoration-none lh-1">
            <img src="images/logo_hri.png" width="50" height="20">
        </a>
        <span class="mb-3 mb-md-0 text-body-secondary">© 2025 Honda Research Institute Japan</span>
    </div>
</footer>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/js/bootstrap.min.js"></script>
</body>
</html>